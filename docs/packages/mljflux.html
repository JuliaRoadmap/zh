<!DOCTYPE html>
<html lang="zh">
<head>
	<meta charset="UTF-8"/>
	<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
	<title>包的使用/MLJFlux的使用 - Roadmap</title>
	<meta name="tURL" id="tURL" content="../../"/>
	<meta name="description" content="包的使用/MLJFlux的使用 - Roadmap">
	<script src="../../extra/info.js"></script><script src='https://giscus.app/client.js' data-repo='JuliaRoadmap/zh' data-repo-id='R_kgDOHQYI2Q' data-category='General' data-category-id='DIC_kwDOHQYI2c4CO2c9' data-mapping='pathname' data-reactions-enabled='1' data-emit-metadata='0' data-input-position='top' data-theme='preferred_color_scheme' data-lang='zh-CN' crossorigin='anonymous' async></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../extra/main.js"></script>
	<link id="theme-href" rel="stylesheet" type="text/css" href="../../css/light.css">
	<link rel="stylesheet" type="text/css" href="../../css/general.css">
	<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css"/>
	<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css"/>
	<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css"/>
	<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
</head>
<body>
	<div id="documenter">
		<nav class="docs-sidebar"><a class='docs-logo'><img src='../../assets/images/logo.png' alt='logo' height='96' width='144'></a>
			<div class="docs-package-name">
			<span class="docs-autofit">Roadmap</span>
			</div>
			<ul class="docs-menu"></ul>
		</nav>
		<div class="docs-main">
			<header class="docs-navbar">
				<nav class="breadcrumb">
					<ul class="is-hidden-mobile"><li class="is-active">包的使用 / MLJFlux的使用</li></ul>
					<ul class="is-hidden-tablet"><li class="is-active">包的使用 / MLJFlux的使用</li></ul>
				</nav>
				<div class="docs-right"><a class='docs-edit-link' href='https://github.com/JuliaRoadmap/zh/tree/master/docs/packages/mljflux.md' target='_blank'><span class='docs-label is-hidden-touch'>编辑此页面</span></a>
					<a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="设置"></a>
					<a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a>
				</div>
			</header>
			<article class="content"><h1 id='header-MLJFlux的使用'>MLJFlux的使用<a class='docs-heading-anchor-permalink'></a></h1><p><a href='https://github.com/FluxML/MLJFlux.jl' target='_blank'>MLJFlux</a> 就是 MLJ 框架对 Flux 的封装易于直接使用 Flux</p><p>可以参照这个 <a href='https://playground.tensorflow.org/#activation=relu&regularization=L1&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0.01&noise=0&networkShape=6,6,6,2&seed=0.28619&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false%5D%5D' target='_blank'>PlayGround</a> 来学习</p><p><img src='../../../assets/images/mljflux/1.png' alt='img'></p><p>其中</p><ul><li><p><code>features</code> 表示特征</p></li><li><p><code>hidden layers</code> 表示隐藏层</p></li><li><p><code>learning rate</code> 设置学习率</p></li><li><p><code>activation</code> 设置每个神经元的激活函数</p></li><li><p><code>regularzation</code> 设置正则化方法</p></li><li><p><code>regularzation rate</code> 设置正则化惩罚力度</p></li></ul><p>但在 MLJ 和 MLJFlux 中，你是找不到任何有关学习率的参数设置的，因为已经有方法可以不设置学习率了，具体参考<a href='https://blog.csdn.net/u012526436/article/details/90486021' target='_blank'>这篇文章</a><br />在 MLJFlux 中，有以下几种模型</p><table style='float:center'><thead><tr><td>模型类型</td><td>预测类型</td><td>scitype(x) &lt;: 啥  scitype(y) &lt;: 啥</td><td></td></tr></thead><tbody><tr><td>NeuralNetworkRegressor</td><td>Deterministic</td><td>Table{Continuous} with n_in columns</td><td>AbstractVector{&lt;:Continuous} n_out = 1</td></tr><tr><td>MultitargetNeuralNetworkRegressor</td><td>Deterministic</td><td>Table{Continuous} with n_in columns</td><td>Table(Continuous) with n_out columns</td></tr><tr><td>NeuralNetworkClassifier</td><td>Probabilistic</td><td>Table(Continuous) with n_in columns</td><td>AbstractVector{&lt;:Finite} with n_out classes</td></tr><tr><td>ImageClassifier</td><td>Probabilistic</td><td>AbstractVector(&lt;:Image{W,H}) with n_in = (W, H)</td><td>AbstractVector{&lt;:Finite} with n_out classes</td></tr></tbody></table><p>它们的参数有</p><ul><li><p><code>builder</code> : <code>Default = MLJFlux.Linear(σ=Flux.relu) (regressors) or MLJFlux.Short(n_hidden=0, dropout=0.5, σ=Flux.σ) (classifiers)</code></p></li><li><p><code>optimiser</code> : <code>The optimiser to use for training. Default = Flux.ADAM()</code></p></li><li><p><code>loss</code> : <code>The loss function used for training. Default = Flux.mse (regressors) and Flux.crossentropy (classifiers)</code></p></li><li><p><code>n_epochs</code> : <code>Number of epochs to train for. Default = 10</code></p></li><li><p><code>batch_size</code> : <code>The batch_size for the data. Default = 1</code></p></li><li><p><code>lambda</code> : <code>The regularization strength. Default = 0. Range = [0, ∞)</code></p></li><li><p><code>alpha</code>: <code>The L2/L1 mix of regularization. Default = 0. Range = [0, 1]</code></p></li><li><p><code>rng</code>: <code>The random number generator (RNG) passed to builders, for weight intitialization, for example. Can be any AbstractRNG or the seed (integer) for a MersenneTwister that is reset on every cold restart of model (machine) training. Default = GLOBAL_RNG.</code></p></li><li><p><code>acceleration</code>: <code>Use CUDALibs() for training on GPU; default is CPU1().</code></p></li><li><p><code>optimiser_changes_trigger_retraining</code> : <code>True if fitting an associated machine should trigger retraining from scratch whenever the optimiser changes. Default = false</code></p></li></ul><p>来挑几个用的上的参数来说吧，</p><ul><li><p><code>builder</code> 那个 <code>hidden layer</code> 的表示</p></li><li><p><code>optimiser</code> 代表优化器，<a href='https://fluxml.ai/Flux.jl/stable/training/optimisers/' target='_blank'>具体文档</a></p></li><li><p><code>loss</code> 表示损失函数</p></li><li><p><code>n_epochs</code> 表示训练次数</p></li><li><p><code>batch_size</code> 表示每次拿几个数据一起处理</p></li><li><p><code>lambda</code> 表示正则化惩罚力度</p></li><li><p><code>alpha</code> 表示 l2/l1 混合的比例 ？</p></li></ul><h2 id='header-实例'>实例<a class='docs-heading-anchor-permalink'></a></h2><p>对照上图，以分类问题为例，我们需要新建一个 <code>builder</code> 结构来表示隐藏层</p><ul><li><p>隐藏层有4层</p></li><li><p>每一层分别有6，6，6，2个神经元</p></li><li><p>每个神经元的激活函数是 ReLu</p></li><li><p>正则化方法是l1</p></li><li><p>正则化惩罚力度是 0.01</p></li></ul><p>由此，创建一个 builder</p><div data-lang='jl'><div class='codeblock-header'></div><pre class='codeblock-body language-jl'><code>mutable struct NetworkBuilder &lt;: MLJFlux.Builder
	n1::Int # 第一层神经元数
	n2::Int # 第二层神经元数
	n3::Int # 第三层神经元数
	n4::Int # 第四层神经元数
end
</code></pre></div><br /><p>再为其重载 <code>MLJFlux.build</code> 方法</p><div data-lang='jl'><div class='codeblock-header'></div><pre class='codeblock-body language-jl'><code>function MLJFlux.build(model::NetworkBuilder, rng, nin, nout)
	init = Flux.glorot_uniform(rng)
	return Chain(
		Dense(nin, model.n1, relu, init = init),
		Dense(model.n1, model.n2, relu, init = init),
		Dense(model.n2, model.n3, relu, init = init),
		Dense(model.n3, model.n4, relu, init = init),
		Dense(model.n4, nout, relu, init = init)
	)
end
</code></pre></div><br /><p>其中</p><ul><li><p><code>rng</code> 表示随机数生成器，大概是这个意思</p></li><li><p><code>nin</code> 表示输入的特征个数</p></li><li><p><code>nout</code> 表示输出的结果个数</p></li></ul><p>当然，这些不用我们设置，我们只是提供一个方法实现</p><p>接下来就可以定义模型了</p><div data-lang='jl'><div class='codeblock-header'></div><pre class='codeblock-body language-jl'><code>classifier = NeuralNetworkClassifier(
	builder = NetworkBuilder(6, 6, 6, 2),
	finaliser = softmax,
	epochs = 200,
	batch_size = 10,
	lambda = 0,
	alpha = 0.01
)
</code></pre></div><br /><p>他的训练次数是 <code>epochs</code> 而不是 <code>n_epochs</code> ，不知道是不是文档写错了，大家按照这个来就好了<br />定义完模型，就像 MLJ 模型那样调用就好了</p><h2 id='header-使用 MLJFlux 预测波士顿房价'>使用 MLJFlux 预测波士顿房价<a class='docs-heading-anchor-permalink'></a></h2><p><img src='../../../assets/images/mljflux/2.png' alt='img'></p><h3 id='header-准备'>准备<a class='docs-heading-anchor-permalink'></a></h3><div data-lang='jl'><div class='codeblock-header'></div><pre class='codeblock-body language-jl'><code>using MLJFlux, MLJ, Statistics, Flux, CSV, StableRNGs, Plots
using DataFrames: DataFrame
import Random.seed!;
seed!(123)
rng = StableRNG(123)
plotly()
originData = CSV.read("data/titanic/train.csv", DataFrame)
</code></pre></div><br /><h3 id='header-数据处理'>数据处理<a class='docs-heading-anchor-permalink'></a></h3><div data-lang='jl'><div class='codeblock-header'></div><pre class='codeblock-body language-jl'><code>typeTransformModel!(dataframe::DataFrame) = begin
	if in("Survived", names(dataframe))
		coerce!(dataframe, :Survived =&gt; Multiclass)
	end
	coerce!(dataframe, Count =&gt; Continuous)
	coerce!(dataframe, Textual =&gt; Multiclass)
	return dataframe
end
	
fillMissingModel = FillImputer(
	features=[:Age, :Embarked],
	continuous_fill = e -&gt; skipmissing(e) |&gt; mode,
	finite_fill = e -&gt; skipmissing(e) |&gt; mode)
	newFeatureModel!(dataframe::DataFrame) = begin
	# MODULE FeatureA 聚集 Age, Sex --&gt; 12岁以下儿童以及妇女，12岁以上男性
	feature_filter_a(age, sex) = age &gt;= 12 &amp;&amp; sex == "male" ? "A" : "B"
	dataframe[!, :FeatureA] = map(feature_filter_a, dataframe[!, :Age], dataframe[!, :Sex])
	# MODULE FeatureB 聚集 SibSp, Parch ---&gt; 家庭人员数量
	family_size(number) = begin
		if number == 1
			return 0
		elseif number &gt;= 2 &amp;&amp; number &lt;= 4
			return 1
		else
			return 2
		end
	end
	dataframe[!, :FeatureB] = map(family_size, dataframe[!, :Parch] .+ dataframe[!, :SibSp] .+ 1)
	# MODULE FeatureC log(Fare + 1), encode(Pclass) -&gt; 1, 2, 3  
	dataframe[!, :Fare] = map(floor, log.(dataframe[!, :Fare] .+ 1))
	# TODO don't forget to coerce scitype
	coerce!(dataframe, :FeatureA =&gt; Multiclass, :FeatureB =&gt; Continuous)
	return dataframe
end

encodeModel = OneHotEncoder(features=[:Embarked, :FeatureA])
dropUnusedModel = FeatureSelector(features = [:Age, :Sex, :SibSp, :Parch, :Cabin, :PassengerId, :Name, :Ticket], ignore=true)

transformModel = (
	typeTransformModel!,
	fillMissingModel,
	newFeatureModel!,
	encodeModel,
	dropUnusedModel
)
transformMachine = machine(transformModel, originData)

fit!(transformMachine)
outputData = MLJ.transform(transformMachine, originData)
originSample = CSV.read("data/titanic/test.csv", DataFrame)

# generic typeTransformModel, ignore
fillMissingModel = FillImputer(features=[:Age, :Fare], continuous_fill = e -&gt; skipmissing(e) |&gt; mode)

# generic new feature generate
# generic encode model
# generic drop unused
transformSampleModel = Pipeline(
	typeTransformModel!,
	fillMissingModel,
	newFeatureModel!,
	encodeModel,
	dropUnusedModel
)

transformSampleMachine = machine(transformSampleModel, originSample)
fit!(transformSampleMachine)

outputSample = MLJ.transform(transformSampleMachine, originSample)

Y, X = unpack(outputData, colname -&gt; colname == :Survived, colname -&gt; true)
</code></pre></div><br /><h3 id='header-模型训练'>模型训练<a class='docs-heading-anchor-permalink'></a></h3><div data-lang='jl'><div class='codeblock-header'></div><pre class='codeblock-body language-jl'><code>rng = StableRNG(1234)
trainRow, testRow = partition(eachindex(Y), 0.7, rng=rng)

mutable struct NetworkBuilder &lt;: MLJFlux.Builder
	n1::Int
	n2::Int
	n3::Int
	n4::Int
end
	
function MLJFlux.build(model::NetworkBuilder, rng, nin, nout)
	init = Flux.glorot_uniform(rng)
	return Chain(
		Dense(nin, model.n1, relu, init = init),
		Dense(model.n1, model.n2, relu, init = init),
		Dense(model.n2, model.n3, relu, init = init),
		Dense(model.n3, model.n4, relu, init = init),
		Dense(model.n4, nout, relu, init = init)
	)
end
	
classifier = NeuralNetworkClassifier(
	builder = NetworkBuilder(10, 6, 6, 6),
	finaliser = softmax,
	epochs = 200,
	batch_size = 10,
	lambda = 0.01,
	alpha = 0.4
)

mach = machine(classifier, X, Y)
fit!(mach, rows = trainRow)

measure = evaluate!(mach,
	resampling = CV(nfolds = 6, rng = rng),
	measure = cross_entropy,
	rows = testRow
)
</code></pre></div><br /><h3 id='header-导出结果'>导出结果<a class='docs-heading-anchor-permalink'></a></h3><div data-lang='jl'><div class='codeblock-header'></div><pre class='codeblock-body language-jl'><code>outputPredict = mode.(predict(mach, outputSample)) |&gt; nums -&gt; convert(Vector{Int}, nums)
output_frame = DataFrame()
output_frame[!, :PassengerId] = convert(Vector{Int}, originSample[!, :PassengerId])
output_frame[!, :Survived] = outputPredict
CSV.write("data/titanic/predict.csv", output_frame)
</code></pre></div><br /></article>
			<nav class="docs-footer"><a class="docs-footer-prevpage" href="index.html">« 索引</a><div class='flexbox-break'></div><p class='footer-message'>Powered by <a href='https://github.com/JuliaRoadmap/DoctreePages.jl'>DoctreePages.jl</a> and its dependencies.</p></nav>
			<div class='giscus'></div>
		</div>
	</div>
</body>
</html>
